{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # R-MINI-CONDA\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get current working directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputLanguage = \"english\"\n",
    "nlp = spacy.load('de_core_news_lg') # en_core_web_lg\n",
    "LOADFILE = \"C:\\\\Users\\\\fenn\\Desktop\\\\tryPython\\\\summarizedWords_LK.txt\"\n",
    "SAVEFILE = \"C:\\\\Users\\\\fenn\\Desktop\\\\tryPython\\\\distanceMatrix_LK.txt\"  # change according to your needs\n",
    "#print(pd.read_csv(LOADFILE, delimiter=\"\\t\").iloc[:, 0])  # for debug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = pd.read_csv(LOADFILE, delimiter=\"\\t\").iloc[:, 0].str.cat(others=None, sep=\" \", na_rep=None, join='left') # reads in the file specified by LOADFILE, converting it to a single string of words separated by a blankspace\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokens = nlp(rawData)\n",
    "print(len(tokens))\n",
    "##cleans up data olny for concepts in the model\n",
    "# Defined as a function to be used on multiple datasets if necessary \n",
    "def cleanData(data):\n",
    "    cleanTokens = []\n",
    "    for token in tokens:\n",
    "        if not token.is_oov:\n",
    "            cleanTokens.append(token) \n",
    "    return cleanTokens\n",
    "\n",
    "cleanTokens = cleanData(tokens)\n",
    "print(cleanTokens)\n",
    "print(len(cleanTokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determining string distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates a matrix of word similarities\n",
    "def calcDistanceMatrix(cleanTokens):\n",
    "    df = pd.DataFrame(columns=cleanTokens, index = cleanTokens) # builds al empty pandas dataframe with rows and columns named after the words to be analyzed\n",
    "    for token in cleanTokens:\n",
    "        similiarities = []\n",
    "        for token2 in cleanTokens:\n",
    "            similiarities.append(token.similarity(token2))\n",
    "        df[token] = similiarities\n",
    "    return df\n",
    "\n",
    "distanceMatrixDF = calcDistanceMatrix(cleanTokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the Matrix (for testing/debug only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distanceMatrixDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a .txt file to/with the location specified by the SAVEFILE variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export DataFrame to text file\n",
    "with open(SAVEFILE, 'w') as f:  # overwrites existing files of the same name and path. If you want to change that: change line to with open(SAVEFILE, 'x') as f:\n",
    "    distMatString = distanceMatrixDF.to_string(header=True, index=True)\n",
    "    f.write(distMatString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion. I like apples. Apple is a big company. New York, Red Flag, I have a red flag.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label, ent.label_)\n",
    "\n",
    "print(nlp.get_pipe('ner').labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def most_similar(word, topn=5):\n",
    "  word = nlp.vocab[str(word)]\n",
    "  queries = [\n",
    "      w for w in word.vocab \n",
    "      if w.is_lower == word.is_lower  and np.count_nonzero(w.vector)\n",
    "  ]\n",
    "\n",
    "  by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "  return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
    "\n",
    "print(most_similar(\"dog\", topn=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of nodes in each network\n",
    "num_nodes = 50\n",
    "\n",
    "# Probability of edge creation for Erdős-Rényi graphs\n",
    "p = 0.2\n",
    "\n",
    "# Generate three random networks\n",
    "networks = [nx.erdos_renyi_graph(num_nodes, p) for _ in range(3)]\n",
    "\n",
    "# Plot the degree distributions on a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, G in enumerate(networks):\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = list(degrees.values())\n",
    "    print(sorted(degree_values))\n",
    "\n",
    "    plt.plot(sorted(degree_values), label=f'Network {i + 1}', marker='o')\n",
    "\n",
    "plt.title('Degree Distributions of Multiple Networks')\n",
    "plt.xlabel('Sorted Nodes')\n",
    "plt.ylabel('Degree')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the degree distributions on a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, G in enumerate(networks):\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = sorted(list(degrees.values()))\n",
    "    print(degree_values)\n",
    "\n",
    "    # Calculate degree distribution\n",
    "    hist, bins = np.histogram(degree_values, bins=range(1, max(degree_values) + 2), density=True)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2.\n",
    "\n",
    "    # Plot degree distribution on log-log scale\n",
    "    plt.loglog(bin_centers, hist, 'o', label=f'Network {i + 1}')\n",
    "\n",
    "plt.title('Degree Distributions of Multiple Networks')\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate a scale-free network\n",
    "scale_free_network = nx.scale_free_graph(1000, alpha=0.3, beta=0.2, gamma=0.5)\n",
    "\n",
    "# Get degrees of all nodes\n",
    "degrees = dict(scale_free_network.degree())\n",
    "\n",
    "# Plot the degree distribution on a log-log scale\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Convert degrees to a numpy array for easier manipulation\n",
    "degree_values = np.array(list(degrees.values()))\n",
    "\n",
    "# Calculate the histogram of degrees\n",
    "hist, bins = np.histogram(degree_values, bins=20)\n",
    "\n",
    "# Ensure no zero values (log(0) is undefined)\n",
    "hist[hist == 0] = 1\n",
    "\n",
    "# Plot the histogram on a log-log scale\n",
    "plt.loglog(bins[:-1], hist, 'o', label='Degree Distribution')\n",
    "\n",
    "plt.title('Scale-Free Network Degree Distribution')\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume we have a list of networks\n",
    "networks = [nx.gnp_random_graph(100, 0.02), nx.gnp_random_graph(100, 0.02), nx.gnp_random_graph(100, 0.02)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, G in enumerate(networks):\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = sorted(set(degrees.values()))\n",
    "    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]\n",
    "\n",
    "    # Plot degree distribution on log-log scale\n",
    "    plt.loglog(degree_values, histogram, 'o', label=f'Network {i + 1}')\n",
    "\n",
    "plt.title('Degree Distributions of Multiple Networks')\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Let's generate networks using the Barabási–Albert model\n",
    "networks = [nx.barabasi_albert_graph(100, 2), nx.barabasi_albert_graph(100, 2), nx.barabasi_albert_graph(100, 2)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, G in enumerate(networks):\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = sorted(set(degrees.values()))\n",
    "    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]\n",
    "\n",
    "    # Plot degree distribution on log-log scale\n",
    "    plt.loglog(degree_values, histogram, 'o', label=f'Network {i + 1}')\n",
    "\n",
    "plt.title('Degree Distributions of Barabási–Albert Networks')\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Probability (log scale)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Let's generate networks using the Barabási–Albert model\n",
    "networks = [nx.barabasi_albert_graph(100, 2), nx.barabasi_albert_graph(100, 2), nx.barabasi_albert_graph(100, 2)]\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10, 12))\n",
    "\n",
    "for i, G in enumerate(networks):\n",
    "    degrees = dict(G.degree())\n",
    "    degree_values = sorted(set(degrees.values()))\n",
    "    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]\n",
    "\n",
    "    # Plot degree distribution on log-log scale\n",
    "    axs[0].loglog(degree_values, histogram, 'o', label=f'Network {i + 1}')\n",
    "\n",
    "    # Plot degree distribution on normal scale\n",
    "    axs[1].plot(degree_values, histogram, 'o', label=f'Network {i + 1}')\n",
    "\n",
    "axs[0].set_title('Degree Distributions of Barabási–Albert Networks (Log-Log Scale)')\n",
    "axs[0].set_xlabel('Degree (log scale)')\n",
    "axs[0].set_ylabel('Probability (log scale)')\n",
    "\n",
    "axs[1].set_title('Degree Distributions of Barabási–Albert Networks (Normal Scale)')\n",
    "axs[1].set_xlabel('Degree')\n",
    "axs[1].set_ylabel('Probability')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
